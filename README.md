
# ğŸ¤– PDF Chat Assistant â€“ NeMo v1 (GraphRAG Ready)

A **Retrieval-Augmented Generation (RAG)** demo built with **NVIDIA NeMo**, enabling users to upload PDFs and interact with their content using natural language questions.

> âœ… Currently uses: FAISS-based retrieval  
> ğŸš§ Upcoming (v2): GraphRAG integration (graph-based contextual search)

---

## ğŸŒ Overview

This application allows users to:

- Upload one or more PDF files.
- Parse and chunk the text using `PyPDFLoader` and `RecursiveCharacterTextSplitter`.
- Create semantic embeddings using `NVIDIAEmbeddings`.
- Store and search document chunks in a persistent `FAISS` vector index.
- Ask questions and retrieve relevant information grounded in context.
- Get answers using `meta/llama3-70b-instruct` via NVIDIA NeMo.

---

## ğŸ§  What Is RAG?

**Retrieval-Augmented Generation (RAG)** enhances large language models (LLMs) by combining them with an external knowledge base (e.g., PDF documents). Instead of generating from scratch, the LLM retrieves relevant content and answers based on that, which results in:

- Increased accuracy
- Lower hallucination
- Real-time document-based reasoning

---

## ğŸ§° Tech Stack

| Component                | Technology                    |
|--------------------------|-------------------------------|
| ğŸ”— LLM                   | `meta/llama3-70b-instruct` via `ChatNVIDIA` |
| ğŸ“š Document Storage      | FAISS Vector Store            |
| ğŸ“„ PDF Ingestion         | LangChain + `PyPDFLoader`     |
| ğŸ§  Embedding Model       | `NVIDIAEmbeddings`            |
| ğŸ’¬ Frontend              | Streamlit                     |
| ğŸ“Š Chunking Strategy     | `RecursiveCharacterTextSplitter` |
| ğŸ’¾ Persistent Storage    | Local folder with FAISS       |

---

## ğŸš€ Upcoming: GraphRAG (v2)

We are actively working on integrating **GraphRAG**, which will replace FAISS with a graph-based retriever. This upgrade will enable:

- Better context awareness by linking semantically related chunks.
- Graph traversal algorithms for topic-centric exploration.
- Advanced reasoning through node-to-node relationships.

ğŸ› ï¸ *Stay tuned for a major update in the next release!*

---

## ğŸ—ï¸ Architecture Diagram

```mermaid
flowchart TD
    A[ğŸ“ Upload PDFs] --> B[ğŸ“„ PyPDFLoader]
    B --> C[ğŸ”— Chunk via TextSplitter]
    C --> D[ğŸ§  NVIDIA Embeddings]
    D --> E[ğŸ—‚ï¸ FAISS Vector Store Persistent data Base]

    F[â“ User Asks Question] --> G[ğŸ” FAISS Retriever]
    G --> H[ğŸ“¦ Prompt Assembly]
    H --> I[ğŸ¤– meta/llama3-70b-instruct]
    I --> J[ğŸ’¬ Streamlit Answer + Source Chunks]
````

---

## ğŸ§¬ System Workflow

### 1. ğŸ“ Upload PDFs

PDFs are uploaded via the Streamlit sidebar and stored temporarily.

### 2. ğŸ“„ Document Parsing

`PyPDFLoader` loads the document and returns structured content.

### 3. ğŸ”— Text Chunking

Documents are split into overlapping chunks using `RecursiveCharacterTextSplitter`.

### 4. ğŸ§  Embedding

Chunks are embedded using `NVIDIAEmbeddings` to convert text into semantic vectors.

### 5. ğŸ—‚ï¸ FAISS Index

Chunks are saved into FAISS (on-disk), enabling fast vector-based similarity search.

### 6. â“ Question Answering

User questions are embedded and matched against stored vectors.

### 7. ğŸ¤– LLM Generation

Retrieved context is passed into `meta/llama3-70b-instruct`, which generates a grounded answer.

### 8. ğŸ’¬ Answer Display

Streamlit shows the answer along with document sources and latency.

---

## âœ… Features

* ğŸ’¾ **Persistent Vector DB**: Upload once, reuse always.
* ğŸ” **Context-Rich Answers**: Powered by semantic search.
* ğŸ“„ **Multi-Document Support**: Handle multiple PDFs at once.
* ğŸ§  **LLM Integration**: High-quality answers from Llama 3.
* ğŸ”œ **GraphRAG (Next)**: Graph-based reasoning engine (coming soon).

---

## âš™ï¸ Installation

```bash
git clone https://github.com/your-username/nemo-pdf-chat-demo.git
cd nemo-pdf-chat-demo
```

Create a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Create a `.env` file:

```env
NVIDIA_API_KEY=your_nvidia_api_key_here
```

---

## ğŸ§ª Run the App

```bash
streamlit run app.py
```

Then open [http://localhost:8501](http://localhost:8501) in your browser.

---

## ğŸ“¦ File Structure

```
.
â”œâ”€â”€ app.py               # Main Streamlit App
â”œâ”€â”€ vectorstore/         # Stored FAISS DBs (one per PDF)
â”œâ”€â”€ image/               # Logos and favicon
â”œâ”€â”€ .env                 # API Keys
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ README.md            # This file
```

---


ğŸ“¢ **Next Release:** Full GraphRAG integration with semantic graph traversal, node linking, and more!

```
